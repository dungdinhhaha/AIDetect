# ğŸ“ HÆ¯á»šNG DáºªN LÃ€M Dá»° ÃN OBJECT DETECTION CHO Y Táº¾

**Dá»± Ã¡n máº«u:** PhÃ¡t hiá»‡n táº¿ bÃ o ung thÆ° cá»• tá»­ cung (ComparisonDetector)  
**Thá»i gian:** 3-6 thÃ¡ng (part-time)  
**Level:** Intermediate â†’ Advanced

---

## ğŸ“‹ Má»¤C Lá»¤C

1. [Chuáº©n bá»‹ & LÃªn káº¿ hoáº¡ch](#bÆ°á»›c-1-chuáº©n-bá»‹--lÃªn-káº¿-hoáº¡ch)
2. [Thu tháº­p dá»¯ liá»‡u](#bÆ°á»›c-2-thu-tháº­p-dá»¯-liá»‡u)
3. [GÃ¡n nhÃ£n dá»¯ liá»‡u](#bÆ°á»›c-3-gÃ¡n-nhÃ£n-dá»¯-liá»‡u)
4. [Setup mÃ´i trÆ°á»ng](#bÆ°á»›c-4-setup-mÃ´i-trÆ°á»ng)
5. [Táº¡o data pipeline](#bÆ°á»›c-5-táº¡o-data-pipeline)
6. [XÃ¢y dá»±ng model](#bÆ°á»›c-6-xÃ¢y-dá»±ng-model)
7. [Training](#bÆ°á»›c-7-training)
8. [Evaluation](#bÆ°á»›c-8-evaluation)
9. [Deployment](#bÆ°á»›c-9-deployment)
10. [Tá»‘i Æ°u hÃ³a](#bÆ°á»›c-10-tá»‘i-Æ°u-hÃ³a)

---

# BÆ¯á»šC 1: CHUáº¨N Bá»Š & LÃŠN Káº¾ HOáº CH

## 1.1. Chá»n bÃ i toÃ¡n cá»¥ thá»ƒ

### âœ… **CÃ¡c Ã½ tÆ°á»Ÿng dá»± Ã¡n tÆ°Æ¡ng tá»±:**

#### **A. Y táº¿ (Medical Imaging):**
```
1. PhÃ¡t hiá»‡n táº¿ bÃ o mÃ¡u báº¥t thÆ°á»ng (Blood Cell Detection)
   - Dataset: BCCD Dataset, Kaggle
   - Classes: WBC, RBC, Platelets, abnormal cells
   - Äá»™ khÃ³: â­â­â­

2. PhÃ¡t hiá»‡n khá»‘i u phá»•i (Lung Nodule Detection)
   - Dataset: LUNA16, NIH Chest X-rays
   - Classes: Nodule, Mass, Normal
   - Äá»™ khÃ³: â­â­â­â­

3. PhÃ¡t hiá»‡n vÃµng máº¡c bá»‡nh tiá»ƒu Ä‘Æ°á»ng (Diabetic Retinopathy)
   - Dataset: Kaggle DR Detection
   - Classes: 5 levels (No DR â†’ Proliferative DR)
   - Äá»™ khÃ³: â­â­â­â­

4. PhÃ¢n loáº¡i táº¿ bÃ o da ung thÆ° (Skin Cancer Classification)
   - Dataset: ISIC Archive
   - Classes: Melanoma, Nevus, Seborrheic Keratosis...
   - Äá»™ khÃ³: â­â­â­
```

#### **B. NÃ´ng nghiá»‡p:**
```
1. PhÃ¡t hiá»‡n sÃ¢u bá»‡nh trÃªn lÃ¡ cÃ¢y
2. Äáº¿m hoa quáº£ trÃªn cÃ¢y (Apple, Orange counting)
3. PhÃ¢n loáº¡i bá»‡nh cÃ¢y trá»“ng
```

#### **C. CÃ´ng nghiá»‡p:**
```
1. PhÃ¡t hiá»‡n lá»—i sáº£n pháº©m (Defect detection)
2. Äáº¿m linh kiá»‡n Ä‘iá»‡n tá»­
3. Kiá»ƒm tra cháº¥t lÆ°á»£ng hÃ n
```

### ğŸ“ **Template chá»n bÃ i toÃ¡n:**

```
[TÃªn dá»± Ã¡n]: _______________________
[Má»¥c tiÃªu]: PhÃ¡t hiá»‡n/PhÃ¢n loáº¡i ______ trong áº£nh _______
[Input]: áº¢nh kÃ­ch thÆ°á»›c ______ x ______
[Output]: 
  - Bounding boxes: [x1, y1, x2, y2]
  - Labels: [Class 1, Class 2, ...]
  - Confidence: 0.0 - 1.0
[Sá»‘ classes]: ______ (+ 1 background)
[Dataset size]: ______ áº£nh (tá»‘i thiá»ƒu 500-1000)
```

---

## 1.2. NghiÃªn cá»©u papers liÃªn quan

### ğŸ“š **Checklist nghiÃªn cá»©u:**

```bash
[ ] Äá»c 3-5 papers vá» bÃ i toÃ¡n tÆ°Æ¡ng tá»±
[ ] TÃ¬m baseline model (YOLOv5, Faster R-CNN, RetinaNet)
[ ] Xem code implementation trÃªn GitHub
[ ] Äá»c discussion trÃªn Kaggle (náº¿u cÃ³ competition)
[ ] TÃ¬m pretrained models
```

### ğŸ” **Websites tÃ¬m papers:**
```
- paperswithcode.com
- arxiv.org
- Google Scholar
- Kaggle Notebooks (Code + Discussion)
```

---

## 1.3. LÃªn timeline

### ğŸ“… **Timeline máº«u (6 thÃ¡ng):**

```
ThÃ¡ng 1: Thu tháº­p + GÃ¡n nhÃ£n dá»¯ liá»‡u (50%)
ThÃ¡ng 2: GÃ¡n nhÃ£n tiáº¿p (50%) + Setup mÃ´i trÆ°á»ng
ThÃ¡ng 3: Data pipeline + Baseline model
ThÃ¡ng 4: Training + Debugging
ThÃ¡ng 5: Tá»‘i Æ°u model + Evaluation
ThÃ¡ng 6: Deployment + Viáº¿t bÃ¡o cÃ¡o
```

---

# BÆ¯á»šC 2: THU THáº¬P Dá»® LIá»†U

## 2.1. TÃ¬m dataset public

### ğŸŒ **Nguá»“n dataset:**

```
1. Kaggle Datasets
   - https://www.kaggle.com/datasets
   - Search: "[your problem] detection dataset"

2. Papers with Code Datasets
   - https://paperswithcode.com/datasets
   - Filter by task: Object Detection

3. Medical-specific:
   - NIH Clinical Center: https://nihcc.app.box.com/v/ChestXray-NIHCC
   - Grand Challenges: https://grand-challenge.org/
   - ISBI Challenges: https://biomedicalimaging.org/

4. Roboflow Universe
   - https://universe.roboflow.com/
   - Pre-annotated datasets

5. GitHub Awesome Lists
   - "awesome-medical-imaging"
   - "awesome-object-detection"
```

### âœ… **Checklist Ä‘Ã¡nh giÃ¡ dataset:**

```
[ ] Äá»§ sá»‘ lÆ°á»£ng: Tá»‘i thiá»ƒu 500-1000 áº£nh
[ ] Cháº¥t lÆ°á»£ng tá»‘t: KhÃ´ng blur, Ä‘á»§ sÃ¡ng
[ ] CÃ³ annotations: Bounding boxes + labels
[ ] License cho phÃ©p sá»­ dá»¥ng
[ ] Train/Test split cÃ³ sáºµn
[ ] Balanced classes (khÃ´ng lá»‡ch quÃ¡ nhiá»u)
```

---

## 2.2. Thu tháº­p dá»¯ liá»‡u riÃªng (náº¿u cáº§n)

### ğŸ“¸ **HÆ°á»›ng dáº«n chá»¥p áº£nh:**

```python
# Quy táº¯c chá»¥p áº£nh y táº¿:
1. Resolution: Tá»‘i thiá»ƒu 1024x1024 pixels
2. Lighting: Äá»“ng Ä‘á»u, khÃ´ng cÃ³ shadow
3. Focus: RÃµ nÃ©t, khÃ´ng blur
4. Background: Sáº¡ch, Ä‘Æ¡n giáº£n
5. Angle: Nháº¥t quÃ¡n, tháº³ng gÃ³c
6. Sá»‘ lÆ°á»£ng: 
   - Má»—i class: Tá»‘i thiá»ƒu 100 áº£nh
   - Total: 500-2000 áº£nh

# Tool:
- Smartphone camera (12MP+)
- Microscope camera (cho medical)
- Scanner (cho slide)
```

### ğŸ“‚ **Cáº¥u trÃºc thÆ° má»¥c:**

```
raw_data/
â”œâ”€â”€ class_1/
â”‚   â”œâ”€â”€ img_001.jpg
â”‚   â”œâ”€â”€ img_002.jpg
â”‚   â””â”€â”€ ...
â”œâ”€â”€ class_2/
â”‚   â”œâ”€â”€ img_001.jpg
â”‚   â””â”€â”€ ...
â””â”€â”€ metadata.csv
```

---

# BÆ¯á»šC 3: GÃN NHÃƒN Dá»® LIá»†U

## 3.1. Chá»n cÃ´ng cá»¥ annotation

### ğŸ› ï¸ **CÃ´ng cá»¥ miá»…n phÃ­:**

| Tool | Pros | Cons | Link |
|------|------|------|------|
| **LabelImg** | ÄÆ¡n giáº£n, offline | Cháº­m cho nhiá»u áº£nh | GitHub |
| **CVAT** | Web-based, team | Cáº§n setup server | cvat.org |
| **Roboflow** | Auto-suggest, cloud | Free cÃ³ giá»›i háº¡n | roboflow.com |
| **VGG Image Annotator** | Lightweight | UI cÅ© | vgg.ox.ac.uk |
| **LabelMe** | Polygon support | Cáº§n Python | GitHub |

### ğŸ’¡ **Khuyáº¿n nghá»‹:**
```
Solo project â†’ LabelImg hoáº·c Roboflow
Team project â†’ CVAT
Medical imaging â†’ QuPath (cho WSI)
```

---

## 3.2. HÆ°á»›ng dáº«n gÃ¡n nhÃ£n

### ğŸ“ **Quy táº¯c váº½ bounding box:**

```
1. Box CHU bá»c toÃ n bá»™ Ä‘á»‘i tÆ°á»£ng
2. KhÃ´ng bá» sÃ³t pháº§n nÃ o (tay, chÃ¢n, viá»n...)
3. KhÃ´ng váº½ quÃ¡ rá»™ng (chá»«a khoáº£ng trá»‘ng)
4. Nháº¥t quÃ¡n vá» kÃ­ch thÆ°á»›c padding
5. Vá»›i Ä‘á»‘i tÆ°á»£ng bá»‹ che khuáº¥t: Váº«n váº½ pháº§n nhÃ¬n tháº¥y

VÃ­ dá»¥:
âŒ Box quÃ¡ nhá»: [x: 10, y: 10, w: 30, h: 30] â†’ thiáº¿u viá»n
âœ… Box vá»«a Ä‘á»§: [x: 5, y: 5, w: 40, h: 40] â†’ bao háº¿t object
âŒ Box quÃ¡ lá»›n: [x: 0, y: 0, w: 60, h: 60] â†’ nhiá»u background
```

### ğŸ¯ **Quality control:**

```python
# Checklist review:
[ ] Má»—i object cÃ³ Ä‘Ãºng 1 box
[ ] KhÃ´ng overlap giá»¯a cÃ¡c class khÃ¡c nhau
[ ] Label chÃ­nh xÃ¡c 100%
[ ] Tá»a Ä‘á»™ náº±m trong áº£nh
[ ] Box khÃ´ng quÃ¡ nhá» (< 10x10 pixels)
[ ] Random check 10% annotations
```

---

## 3.3. Convert annotations sang COCO/PASCAL VOC format

### ğŸ“¦ **COCO Format (Khuyáº¿n nghá»‹):**

```json
{
  "images": [
    {
      "id": 1,
      "file_name": "img_001.jpg",
      "width": 640,
      "height": 640
    }
  ],
  "annotations": [
    {
      "id": 1,
      "image_id": 1,
      "category_id": 1,
      "bbox": [x, y, width, height],
      "area": width * height,
      "iscrowd": 0
    }
  ],
  "categories": [
    {"id": 1, "name": "cell_type_1"},
    {"id": 2, "name": "cell_type_2"}
  ]
}
```

### ğŸ”„ **Script convert:**

```python
# convert_to_coco.py
import json
import os
from PIL import Image

def labelimg_to_coco(labelimg_folder, output_json):
    """Convert LabelImg XML to COCO JSON"""
    
    coco = {
        "images": [],
        "annotations": [],
        "categories": []
    }
    
    # Add categories
    categories = ["background", "class_1", "class_2"]  # Thay Ä‘á»•i
    for i, cat in enumerate(categories):
        coco["categories"].append({
            "id": i,
            "name": cat
        })
    
    ann_id = 1
    for img_id, xml_file in enumerate(os.listdir(labelimg_folder)):
        if not xml_file.endswith('.xml'):
            continue
        
        # Parse XML (dÃ¹ng xml.etree.ElementTree)
        # ... (xem full code trong project)
        
        coco["images"].append({
            "id": img_id,
            "file_name": image_filename,
            "width": width,
            "height": height
        })
        
        for box in boxes:
            coco["annotations"].append({
                "id": ann_id,
                "image_id": img_id,
                "category_id": box['category_id'],
                "bbox": [box['x'], box['y'], box['w'], box['h']],
                "area": box['w'] * box['h'],
                "iscrowd": 0
            })
            ann_id += 1
    
    with open(output_json, 'w') as f:
        json.dump(coco, f, indent=2)

# Usage:
labelimg_to_coco("annotations/", "coco_annotations.json")
```

---

# BÆ¯á»šC 4: SETUP MÃ”I TRÆ¯á»œNG

## 4.1. CÃ i Ä‘áº·t Python vÃ  dependencies

### ğŸ **Python environment:**

```bash
# 1. CÃ i Anaconda hoáº·c Miniconda
# Download: https://www.anaconda.com/

# 2. Táº¡o virtual environment
conda create -n cell_detection python=3.10
conda activate cell_detection

# 3. CÃ i TensorFlow (GPU)
pip install tensorflow==2.19.0
pip install tensorflow-gpu  # Náº¿u cÃ³ NVIDIA GPU

# 4. CÃ i libraries
pip install opencv-python-headless
pip install pillow
pip install matplotlib
pip install scikit-image
pip install scipy
pip install tqdm
pip install pycocotools

# 5. Kiá»ƒm tra
python -c "import tensorflow as tf; print(tf.__version__); print(tf.config.list_physical_devices('GPU'))"
```

---

## 4.2. Cáº¥u trÃºc project

### ğŸ“ **Template structure:**

```
my_detection_project/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â”œâ”€â”€ config.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # áº¢nh gá»‘c
â”‚   â”œâ”€â”€ annotations/            # XML/JSON annotations
â”‚   â”œâ”€â”€ tfrecords/             # TFRecord files
â”‚   â”‚   â”œâ”€â”€ train.tfrecord
â”‚   â”‚   â””â”€â”€ test.tfrecord
â”‚   â””â”€â”€ splits/
â”‚       â”œâ”€â”€ train.txt
â”‚       â””â”€â”€ test.txt
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ backbone.py            # ResNet, VGG...
â”‚   â”œâ”€â”€ fpn.py                 # Feature Pyramid
â”‚   â”œâ”€â”€ rpn.py                 # Region Proposal Network
â”‚   â”œâ”€â”€ detector.py            # Main model
â”‚   â””â”€â”€ losses.py              # Loss functions
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_loader.py         # Load TFRecord
â”‚   â”œâ”€â”€ augmentation.py        # Data augmentation
â”‚   â”œâ”€â”€ box_utils.py           # IoU, NMS...
â”‚   â””â”€â”€ visualize.py           # Váº½ boxes
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ prepare_data.py        # Convert to TFRecord
â”‚   â”œâ”€â”€ train.py               # Training script
â”‚   â”œâ”€â”€ evaluate.py            # Evaluation
â”‚   â””â”€â”€ inference.py           # Prediction
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_model_test.ipynb
â”‚   â””â”€â”€ 03_results_analysis.ipynb
â”‚
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ models/                # Saved models
â”‚   â”œâ”€â”€ logs/                  # TensorBoard logs
â”‚   â””â”€â”€ predictions/           # Inference results
â”‚
â””â”€â”€ deployment/
    â”œâ”€â”€ api/
    â”‚   â”œâ”€â”€ app.py             # FastAPI
    â”‚   â””â”€â”€ Dockerfile
    â””â”€â”€ web/
        â””â”€â”€ index.html         # Demo UI
```

---

## 4.3. Config file template

### âš™ï¸ **config.py:**

```python
import os

class Config:
    # Project
    PROJECT_NAME = "my_cell_detection"
    
    # Paths
    BASE_DIR = os.path.dirname(os.path.abspath(__file__))
    DATA_DIR = os.path.join(BASE_DIR, "data")
    RAW_DATA_DIR = os.path.join(DATA_DIR, "raw")
    TFRECORD_DIR = os.path.join(DATA_DIR, "tfrecords")
    MODEL_DIR = os.path.join(BASE_DIR, "outputs", "models")
    LOG_DIR = os.path.join(BASE_DIR, "outputs", "logs")
    
    # Dataset
    NUM_CLASSES = 3  # Sá»‘ classes + background
    CLASS_NAMES = ["background", "class_1", "class_2"]
    IMAGE_SIZE = (640, 640)  # (height, width)
    TRAIN_SPLIT = 0.8  # 80% train, 20% test
    
    # Model
    BACKBONE = "resnet50"  # resnet50, resnet101, efficientnet
    BACKBONE_WEIGHTS = "imagenet"
    FPN_CHANNELS = 256
    
    # Training
    BATCH_SIZE = 4
    EPOCHS = 50
    LEARNING_RATE = 1e-3
    MOMENTUM = 0.9
    WEIGHT_DECAY = 1e-4
    
    # Anchors
    ANCHOR_SCALES = [32, 64, 128, 256, 512]
    ANCHOR_RATIOS = [0.5, 1.0, 2.0]
    
    # Inference
    CONFIDENCE_THRESHOLD = 0.5
    NMS_IOU_THRESHOLD = 0.3
    MAX_DETECTIONS = 100
    
    # Augmentation
    USE_AUGMENTATION = True
    FLIP_HORIZONTAL = True
    FLIP_VERTICAL = False
    ROTATION_RANGE = 10  # degrees
    BRIGHTNESS_RANGE = 0.2
    
    # GPU
    USE_GPU = True
    MIXED_PRECISION = True  # Faster training

# Usage:
cfg = Config()
```

---

# BÆ¯á»šC 5: Táº O DATA PIPELINE

## 5.1. Convert to TFRecord

### ğŸ“¦ **Script: `prepare_data.py`**

```python
import tensorflow as tf
import json
import cv2
import numpy as np
from config import Config

cfg = Config()

def create_tfrecord_from_coco(coco_json, image_dir, output_file):
    """
    Convert COCO annotations to TFRecord
    
    Args:
        coco_json: Path to COCO JSON file
        image_dir: Directory containing images
        output_file: Output TFRecord file
    """
    
    # Load COCO
    with open(coco_json) as f:
        coco = json.load(f)
    
    # Create category mapping
    cat_id_to_label = {cat['id']: i for i, cat in enumerate(coco['categories'])}
    
    # Group annotations by image
    img_to_anns = {}
    for ann in coco['annotations']:
        img_id = ann['image_id']
        if img_id not in img_to_anns:
            img_to_anns[img_id] = []
        img_to_anns[img_id].append(ann)
    
    # Write TFRecord
    with tf.io.TFRecordWriter(output_file) as writer:
        for img_info in coco['images']:
            img_id = img_info['id']
            img_path = os.path.join(image_dir, img_info['file_name'])
            
            # Read image
            img = cv2.imread(img_path)
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            img_bytes = img.tobytes()
            
            # Get annotations
            anns = img_to_anns.get(img_id, [])
            
            boxes = []
            labels = []
            for ann in anns:
                x, y, w, h = ann['bbox']
                boxes.extend([x, y, x+w, y+h])
                labels.append(cat_id_to_label[ann['category_id']])
            
            # Create TFRecord example
            feature = {
                'img': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_bytes])),
                'img_height': tf.train.Feature(int64_list=tf.train.Int64List(value=[img.shape[0]])),
                'img_width': tf.train.Feature(int64_list=tf.train.Int64List(value=[img.shape[1]])),
                'gtboxes_and_label': tf.train.Feature(
                    bytes_list=tf.train.BytesList(
                        value=[np.array(boxes + labels, dtype=np.int32).tobytes()]
                    )
                ),
                'img_name': tf.train.Feature(
                    bytes_list=tf.train.BytesList(value=[img_info['file_name'].encode()])
                )
            }
            
            example = tf.train.Example(features=tf.train.Features(feature=feature))
            writer.write(example.SerializeToString())
            
    print(f"âœ“ Created {output_file}")

# Usage:
create_tfrecord_from_coco(
    "data/annotations/train.json",
    "data/raw/images/",
    "data/tfrecords/train.tfrecord"
)
```

---

## 5.2. Data loader

### ğŸ”„ **Script: `utils/data_loader.py`**

```python
import tensorflow as tf
from config import Config

cfg = Config()

def parse_tfrecord(example_proto):
    """Parse TFRecord example"""
    
    features = {
        'img': tf.io.FixedLenFeature([], tf.string),
        'img_height': tf.io.FixedLenFeature([], tf.int64),
        'img_width': tf.io.FixedLenFeature([], tf.int64),
        'gtboxes_and_label': tf.io.FixedLenFeature([], tf.string),
        'img_name': tf.io.FixedLenFeature([], tf.string),
    }
    
    parsed = tf.io.parse_single_example(example_proto, features)
    
    # Decode image
    height = tf.cast(parsed['img_height'], tf.int32)
    width = tf.cast(parsed['img_width'], tf.int32)
    
    image = tf.io.decode_raw(parsed['img'], tf.uint8)
    image = tf.reshape(image, [height, width, 3])
    
    # Decode boxes and labels
    gtboxes_and_label = tf.io.decode_raw(parsed['gtboxes_and_label'], tf.int32)
    num_boxes = tf.shape(gtboxes_and_label)[0] // 5
    gtboxes_and_label = tf.reshape(gtboxes_and_label, [num_boxes, 5])
    
    boxes = tf.cast(gtboxes_and_label[:, :4], tf.float32)
    labels = gtboxes_and_label[:, 4]
    
    return image, {'boxes': boxes, 'labels': labels}

def preprocess(image, targets, image_size=(640, 640)):
    """Preprocess image and targets"""
    
    # Resize image
    image = tf.cast(image, tf.float32) / 255.0
    image = tf.image.resize(image, image_size)
    
    # Normalize boxes to [0, 1]
    # TODO: Implement box normalization
    
    return image, targets

def augment(image, targets):
    """Data augmentation"""
    
    if cfg.USE_AUGMENTATION:
        # Random flip
        if cfg.FLIP_HORIZONTAL:
            if tf.random.uniform(()) > 0.5:
                image = tf.image.flip_left_right(image)
                # TODO: Flip boxes coordinates
        
        # Random brightness
        image = tf.image.random_brightness(image, cfg.BRIGHTNESS_RANGE)
        
        # Random rotation (advanced)
        # TODO: Implement rotation with box transformation
    
    return image, targets

def build_dataset(tfrecord_paths, batch_size=4, shuffle=True, augment=False):
    """Build training/validation dataset"""
    
    AUTOTUNE = tf.data.AUTOTUNE
    
    # Load TFRecord
    dataset = tf.data.TFRecordDataset(tfrecord_paths)
    
    # Parse
    dataset = dataset.map(parse_tfrecord, num_parallel_calls=AUTOTUNE)
    
    # Augment
    if augment:
        dataset = dataset.map(
            lambda img, tgt: augment(img, tgt),
            num_parallel_calls=AUTOTUNE
        )
    
    # Preprocess
    dataset = dataset.map(
        lambda img, tgt: preprocess(img, tgt, cfg.IMAGE_SIZE),
        num_parallel_calls=AUTOTUNE
    )
    
    # Shuffle and batch
    if shuffle:
        dataset = dataset.shuffle(1000)
    
    dataset = dataset.batch(batch_size)
    dataset = dataset.prefetch(AUTOTUNE)
    
    return dataset
```

---

# BÆ¯á»šC 6: XÃ‚Y Dá»°NG MODEL

## 6.1. Backbone (Feature Extractor)

### ğŸ§  **Script: `models/backbone.py`**

```python
import tensorflow as tf
from tensorflow.keras import layers, Model
from tensorflow.keras.applications import ResNet50, ResNet101

def build_backbone(name='resnet50', weights='imagenet'):
    """
    Build feature extraction backbone
    
    Args:
        name: 'resnet50', 'resnet101', 'efficientnetb0'...
        weights: 'imagenet' or None
    
    Returns:
        Keras Model vá»›i multi-scale outputs
    """
    
    if name == 'resnet50':
        base_model = ResNet50(
            include_top=False,
            weights=weights,
            input_shape=(None, None, 3)
        )
    elif name == 'resnet101':
        base_model = ResNet101(
            include_top=False,
            weights=weights,
            input_shape=(None, None, 3)
        )
    else:
        raise ValueError(f"Unknown backbone: {name}")
    
    # Extract multi-scale features
    # C3, C4, C5 (stride 8, 16, 32)
    
    layer_names = {
        'resnet50': ['conv3_block4_out', 'conv4_block6_out', 'conv5_block3_out'],
        'resnet101': ['conv3_block4_out', 'conv4_block23_out', 'conv5_block3_out']
    }
    
    outputs = [base_model.get_layer(name).output for name in layer_names[name]]
    
    backbone = Model(inputs=base_model.input, outputs=outputs, name=f'{name}_backbone')
    
    return backbone
```

---

## 6.2. Feature Pyramid Network (FPN)

### ğŸ”º **Script: `models/fpn.py`**

```python
class FPN(tf.keras.layers.Layer):
    """Feature Pyramid Network"""
    
    def __init__(self, channels=256, **kwargs):
        super().__init__(**kwargs)
        self.channels = channels
        
        # Lateral convolutions (1x1)
        self.lateral_c5 = layers.Conv2D(channels, 1, name='lateral_c5')
        self.lateral_c4 = layers.Conv2D(channels, 1, name='lateral_c4')
        self.lateral_c3 = layers.Conv2D(channels, 1, name='lateral_c3')
        
        # Output convolutions (3x3)
        self.output_p5 = layers.Conv2D(channels, 3, padding='same', name='output_p5')
        self.output_p4 = layers.Conv2D(channels, 3, padding='same', name='output_p4')
        self.output_p3 = layers.Conv2D(channels, 3, padding='same', name='output_p3')
    
    def call(self, features, training=False):
        """
        Args:
            features: [C3, C4, C5] tá»« backbone
        Returns:
            pyramid: [P3, P4, P5] feature maps
        """
        c3, c4, c5 = features
        
        # Top-down pathway
        p5 = self.lateral_c5(c5)
        p4 = self.lateral_c4(c4) + tf.image.resize(p5, tf.shape(c4)[1:3])
        p3 = self.lateral_c3(c3) + tf.image.resize(p4, tf.shape(c3)[1:3])
        
        # Refine with 3x3 conv
        p5 = self.output_p5(p5)
        p4 = self.output_p4(p4)
        p3 = self.output_p3(p3)
        
        return [p3, p4, p5]
```

---

## 6.3. Region Proposal Network (RPN)

### ğŸ¯ **Script: `models/rpn.py`**

```python
class RPN(tf.keras.layers.Layer):
    """Region Proposal Network"""
    
    def __init__(self, channels=256, num_anchors=9, **kwargs):
        super().__init__(**kwargs)
        self.channels = channels
        self.num_anchors = num_anchors
        
        # Shared conv
        self.conv = layers.Conv2D(channels, 3, padding='same', activation='relu')
        
        # Objectness (cÃ³ object hay khÃ´ng)
        self.objectness = layers.Conv2D(num_anchors, 1, name='rpn_objectness')
        
        # Bounding box regression
        self.bbox_reg = layers.Conv2D(num_anchors * 4, 1, name='rpn_bbox')
    
    def call(self, pyramid, training=False):
        """
        Args:
            pyramid: [P3, P4, P5]
        Returns:
            objectness_logits: List of [B, H, W, num_anchors]
            bbox_deltas: List of [B, H, W, num_anchors*4]
        """
        objectness_logits = []
        bbox_deltas = []
        
        for p in pyramid:
            # Shared conv
            x = self.conv(p)
            
            # Predictions
            obj = self.objectness(x)
            bbox = self.bbox_reg(x)
            
            objectness_logits.append(obj)
            bbox_deltas.append(bbox)
        
        return objectness_logits, bbox_deltas
```

---

## 6.4. Main Detector

### ğŸ¯ **Script: `models/detector.py`**

```python
from models.backbone import build_backbone
from models.fpn import FPN
from models.rpn import RPN
from utils.box_utils import nms

class Detector(tf.keras.Model):
    """Complete object detection model"""
    
    def __init__(self, num_classes, backbone_name='resnet50', **kwargs):
        super().__init__(**kwargs)
        
        self.num_classes = num_classes
        
        # Components
        self.backbone = build_backbone(backbone_name)
        self.fpn = FPN(channels=256)
        self.rpn = RPN(channels=256, num_anchors=9)
        
        # Fast R-CNN head (simplified)
        self.roi_align = layers.Lambda(lambda x: x)  # TODO: Implement
        self.classifier = layers.Dense(num_classes, activation='softmax')
    
    def call(self, inputs, training=False):
        """
        Args:
            inputs: [B, H, W, 3] images
        Returns:
            boxes: [N, 4] detected boxes
            labels: [N] class labels
            scores: [N] confidence scores
        """
        # Feature extraction
        c3, c4, c5 = self.backbone(inputs)
        
        # FPN
        pyramid = self.fpn([c3, c4, c5], training=training)
        
        # RPN
        objectness, bbox_deltas = self.rpn(pyramid, training=training)
        
        # Generate proposals (simplified)
        # TODO: Implement anchor generation and proposal selection
        
        # For now, return dummy outputs
        boxes = tf.zeros((1, 4))
        labels = tf.zeros((1,), dtype=tf.int32)
        scores = tf.zeros((1,))
        
        return boxes, labels, scores
    
    def predict_on_image(self, image):
        """Inference on single image"""
        # Preprocess
        img = tf.expand_dims(image, 0)
        
        # Predict
        boxes, labels, scores = self(img, training=False)
        
        return boxes.numpy(), labels.numpy(), scores.numpy()
```

---

# BÆ¯á»šC 7: TRAINING

## 7.1. Loss functions

### ğŸ“‰ **Script: `models/losses.py`**

```python
def focal_loss(y_true, y_pred, alpha=0.25, gamma=2.0):
    """
    Focal Loss for imbalanced classification
    
    Args:
        y_true: Ground truth labels [B, N]
        y_pred: Predicted probabilities [B, N, num_classes]
        alpha: Balancing factor
        gamma: Focusing parameter
    """
    epsilon = tf.keras.backend.epsilon()
    y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)
    
    cross_entropy = -y_true * tf.math.log(y_pred)
    weight = alpha * tf.pow(1 - y_pred, gamma)
    
    loss = weight * cross_entropy
    return tf.reduce_sum(loss, axis=-1)

def smooth_l1_loss(y_true, y_pred, sigma=3.0):
    """
    Smooth L1 loss for bounding box regression
    
    Args:
        y_true: Ground truth boxes [B, N, 4]
        y_pred: Predicted boxes [B, N, 4]
        sigma: Smoothing parameter
    """
    diff = tf.abs(y_true - y_pred)
    
    less_than_one = tf.cast(tf.less(diff, 1.0 / sigma ** 2), tf.float32)
    
    smooth_l1 = (less_than_one * 0.5 * sigma ** 2 * tf.pow(diff, 2)) + \
                ((1 - less_than_one) * (diff - 0.5 / sigma ** 2))
    
    return tf.reduce_mean(smooth_l1)

def detection_loss(y_true, y_pred):
    """
    Combined detection loss
    
    Returns:
        total_loss, cls_loss, box_loss
    """
    # Classification loss
    cls_loss = focal_loss(
        y_true['labels'],
        y_pred['class_probs']
    )
    
    # Box regression loss
    box_loss = smooth_l1_loss(
        y_true['boxes'],
        y_pred['box_deltas']
    )
    
    # Total loss
    total_loss = cls_loss + box_loss
    
    return total_loss, cls_loss, box_loss
```

---

## 7.2. Training script

### ğŸ‹ï¸ **Script: `train.py`**

```python
import tensorflow as tf
from tensorflow.keras import optimizers
from config import Config
from utils.data_loader import build_dataset
from models.detector import Detector
from models.losses import detection_loss

cfg = Config()

def train():
    """Main training function"""
    
    # Setup
    os.makedirs(cfg.MODEL_DIR, exist_ok=True)
    os.makedirs(cfg.LOG_DIR, exist_ok=True)
    
    # Build dataset
    train_ds = build_dataset(
        [os.path.join(cfg.TFRECORD_DIR, 'train.tfrecord')],
        batch_size=cfg.BATCH_SIZE,
        shuffle=True,
        augment=True
    )
    
    val_ds = build_dataset(
        [os.path.join(cfg.TFRECORD_DIR, 'test.tfrecord')],
        batch_size=cfg.BATCH_SIZE,
        shuffle=False,
        augment=False
    )
    
    # Build model
    model = Detector(
        num_classes=cfg.NUM_CLASSES,
        backbone_name=cfg.BACKBONE
    )
    
    # Optimizer
    optimizer = optimizers.Adam(learning_rate=cfg.LEARNING_RATE)
    
    # Metrics
    train_loss_metric = tf.keras.metrics.Mean(name='train_loss')
    val_loss_metric = tf.keras.metrics.Mean(name='val_loss')
    
    # TensorBoard
    train_summary_writer = tf.summary.create_file_writer(
        os.path.join(cfg.LOG_DIR, 'train')
    )
    val_summary_writer = tf.summary.create_file_writer(
        os.path.join(cfg.LOG_DIR, 'val')
    )
    
    # Training loop
    for epoch in range(cfg.EPOCHS):
        print(f"\nEpoch {epoch+1}/{cfg.EPOCHS}")
        
        # Train
        train_loss_metric.reset_states()
        for step, (images, targets) in enumerate(train_ds):
            loss = train_step(model, images, targets, optimizer)
            train_loss_metric.update_state(loss)
            
            if step % 10 == 0:
                print(f"Step {step}, Loss: {loss:.4f}")
        
        # Validation
        val_loss_metric.reset_states()
        for images, targets in val_ds:
            loss = val_step(model, images, targets)
            val_loss_metric.update_state(loss)
        
        # Log
        with train_summary_writer.as_default():
            tf.summary.scalar('loss', train_loss_metric.result(), step=epoch)
        
        with val_summary_writer.as_default():
            tf.summary.scalar('loss', val_loss_metric.result(), step=epoch)
        
        print(f"Train Loss: {train_loss_metric.result():.4f}")
        print(f"Val Loss: {val_loss_metric.result():.4f}")
        
        # Save checkpoint
        if (epoch + 1) % 5 == 0:
            model.save_weights(
                os.path.join(cfg.MODEL_DIR, f'model_epoch_{epoch+1}.h5')
            )
    
    # Save final model
    model.save(os.path.join(cfg.MODEL_DIR, 'final_model.keras'))
    print("\nâœ“ Training completed!")

@tf.function
def train_step(model, images, targets, optimizer):
    """Single training step"""
    with tf.GradientTape() as tape:
        predictions = model(images, training=True)
        loss, cls_loss, box_loss = detection_loss(targets, predictions)
    
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
    
    return loss

@tf.function
def val_step(model, images, targets):
    """Single validation step"""
    predictions = model(images, training=False)
    loss, _, _ = detection_loss(targets, predictions)
    return loss

if __name__ == '__main__':
    train()
```

---

# BÆ¯á»šC 8: EVALUATION

## 8.1. Metrics

### ğŸ“Š **Script: `evaluate.py`**

```python
from utils.box_utils import calculate_iou

def calculate_ap(precision, recall):
    """Calculate Average Precision"""
    # 11-point interpolation
    ap = 0
    for t in np.arange(0., 1.1, 0.1):
        if np.sum(recall >= t) == 0:
            p = 0
        else:
            p = np.max(precision[recall >= t])
        ap += p / 11.
    return ap

def evaluate_model(model, test_dataset, iou_threshold=0.5):
    """
    Evaluate model on test set
    
    Returns:
        mAP: Mean Average Precision
        per_class_ap: AP for each class
    """
    
    all_predictions = []
    all_ground_truths = []
    
    # Collect predictions
    for images, targets in test_dataset:
        boxes, labels, scores = model(images, training=False)
        
        all_predictions.append({
            'boxes': boxes.numpy(),
            'labels': labels.numpy(),
            'scores': scores.numpy()
        })
        
        all_ground_truths.append({
            'boxes': targets['boxes'].numpy(),
            'labels': targets['labels'].numpy()
        })
    
    # Calculate AP per class
    aps = []
    for class_id in range(1, cfg.NUM_CLASSES):  # Skip background
        precision, recall = calculate_precision_recall(
            all_predictions,
            all_ground_truths,
            class_id,
            iou_threshold
        )
        
        ap = calculate_ap(precision, recall)
        aps.append(ap)
        
        print(f"Class {cfg.CLASS_NAMES[class_id]}: AP = {ap:.4f}")
    
    mAP = np.mean(aps)
    print(f"\nmAP@{iou_threshold}: {mAP:.4f}")
    
    return mAP, aps

# Run evaluation
model = tf.keras.models.load_model('outputs/models/final_model.keras')
test_ds = build_dataset(['data/tfrecords/test.tfrecord'], batch_size=1)

mAP, aps = evaluate_model(model, test_ds)
```

---

# BÆ¯á»šC 9: DEPLOYMENT

## 9.1. FastAPI Service

### ğŸš€ **Script: `deployment/api/app.py`**

```python
from fastapi import FastAPI, File, UploadFile
from fastapi.responses import JSONResponse
import tensorflow as tf
import numpy as np
from PIL import Image
import io

app = FastAPI(title="Cell Detection API")

# Load model
MODEL_PATH = "../../outputs/models/final_model.keras"
model = tf.keras.models.load_model(MODEL_PATH)

@app.post("/detect")
async def detect_cells(file: UploadFile = File(...)):
    """Detect cells in uploaded image"""
    
    try:
        # Read image
        contents = await file.read()
        image = Image.open(io.BytesIO(contents))
        image = image.convert('RGB')
        image = image.resize((640, 640))
        
        # Preprocess
        img_array = np.array(image) / 255.0
        img_array = np.expand_dims(img_array, 0)
        
        # Predict
        boxes, labels, scores = model.predict(img_array)
        
        # Format response
        detections = []
        for i in range(len(boxes)):
            if scores[i] > 0.5:
                detections.append({
                    "box": boxes[i].tolist(),
                    "label": int(labels[i]),
                    "confidence": float(scores[i])
                })
        
        return {
            "status": "success",
            "total_detections": len(detections),
            "detections": detections
        }
        
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": str(e)}
        )

@app.get("/health")
def health():
    return {"status": "healthy"}

# Run: uvicorn app:app --reload
```

---

## 9.2. Dockerfile

### ğŸ³ **deployment/api/Dockerfile:**

```dockerfile
FROM tensorflow/tensorflow:2.19.0-gpu

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy code
COPY . .

# Expose port
EXPOSE 8000

# Run
CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000"]
```

---

# BÆ¯á»šC 10: Tá»I Æ¯U HÃ“A

## 10.1. Model optimization

### âš¡ **Techniques:**

```python
# 1. Model pruning
import tensorflow_model_optimization as tfmot

pruning_params = {
    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(
        initial_sparsity=0.0,
        final_sparsity=0.5,
        begin_step=0,
        end_step=1000
    )
}

model_for_pruning = tfmot.sparsity.keras.prune_low_magnitude(
    model,
    **pruning_params
)

# 2. Quantization
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()

# 3. TensorRT (NVIDIA GPU)
from tensorflow.python.compiler.tensorrt import trt_convert as trt

converter = trt.TrtGraphConverterV2(
    input_saved_model_dir='saved_model/',
    precision_mode='FP16'
)
converter.convert()
converter.save('tensorrt_model/')
```

---

# ğŸ“š CHECKLIST HOÃ€N THÃ€NH Dá»° ÃN

## Giai Ä‘oáº¡n 1: Data (Tuáº§n 1-8)
```
[ ] Thu tháº­p 500-2000 áº£nh
[ ] GÃ¡n nhÃ£n 100% áº£nh
[ ] Split train/val/test (70/15/15)
[ ] Convert sang TFRecord
[ ] Data augmentation ready
```

## Giai Ä‘oáº¡n 2: Model (Tuáº§n 9-12)
```
[ ] Backbone implementation
[ ] FPN implementation
[ ] RPN implementation
[ ] Detector integration
[ ] Loss functions
```

## Giai Ä‘oáº¡n 3: Training (Tuáº§n 13-16)
```
[ ] Train baseline (ResNet50)
[ ] Monitor TensorBoard
[ ] Checkpoint best model
[ ] Hyperparameter tuning
[ ] Train final model
```

## Giai Ä‘oáº¡n 4: Evaluation (Tuáº§n 17-18)
```
[ ] mAP calculation
[ ] Confusion matrix
[ ] Error analysis
[ ] Compare vá»›i baseline papers
```

## Giai Ä‘oáº¡n 5: Deployment (Tuáº§n 19-20)
```
[ ] FastAPI service
[ ] Docker containerization
[ ] API documentation
[ ] Demo frontend
```

## Giai Ä‘oáº¡n 6: Documentation (Tuáº§n 21-24)
```
[ ] README.md
[ ] Code comments
[ ] API docs
[ ] Technical report/Paper
[ ] Presentation slides
```

---

# ğŸ¯ TIPS THÃ€NH CÃ”NG

## DO:
âœ… Báº¯t Ä‘áº§u vá»›i dataset nhá» (100 áº£nh) Ä‘á»ƒ test pipeline  
âœ… Visualize má»i thá»© (data, predictions, errors)  
âœ… Version control vá»›i Git tá»« Ä‘áº§u  
âœ… Log experiments (MLflow, Weights & Biases)  
âœ… Äá»c papers related works  
âœ… Tham gia communities (Reddit, Discord)  

## DON'T:
âŒ Train ngay trÃªn full dataset mÃ  chÆ°a test  
âŒ Bá» qua data quality check  
âŒ Hardcode paths vÃ  parameters  
âŒ Train quÃ¡ nhiá»u epochs mÃ  khÃ´ng monitor  
âŒ Bá» qua validation set  
âŒ Copy code mÃ  khÃ´ng hiá»ƒu  

---

# ğŸ“ LIÃŠN Há»† & Há»I ÄÃP

Náº¿u gáº·p khÃ³ khÄƒn á»Ÿ bÆ°á»›c nÃ o, hÃ£y:

1. **Google error message** Ä‘áº§u tiÃªn
2. **Check GitHub Issues** cá»§a libraries liÃªn quan
3. **Há»i trÃªn Stack Overflow** vá»›i tag `tensorflow`, `object-detection`
4. **Discord communities:**
   - TensorFlow Discord
   - PyTorch Discord
   - r/MachineLearning
5. **Paper authors:** Email náº¿u cáº§n clarification

---

# ğŸ“ Káº¾T LUáº¬N

Báº¡n vá»«a cÃ³ má»™t roadmap Ä‘áº§y Ä‘á»§ Ä‘á»ƒ tá»± lÃ m dá»± Ã¡n Object Detection tá»« A-Z!

**Nhá»›:**
- LÃ m tá»«ng bÆ°á»›c má»™t, Ä‘á»«ng vá»™i
- Test vÃ  debug liÃªn tá»¥c
- Document code ngay tá»« Ä‘áº§u
- Há»c tá»« errors (chÃºng lÃ  tháº§y tá»‘t nháº¥t!)

**ChÃºc báº¡n thÃ nh cÃ´ng! ğŸš€**

---

**Author:** Based on ComparisonDetector project  
**GitHub:** https://github.com/dungdinhhaha/AIDetect  
**Date:** December 2025
