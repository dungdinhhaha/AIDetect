{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e77232fb",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup & Mount Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bf81f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/dungdinhhaha/AIDetect.git\n",
    "%cd AIDetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939cac67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q tensorflow==2.19.0 matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bd0f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73b1a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU available:\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"Num GPUs:\", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717534e4",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b9d785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available models\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "model_dir = Path('/content/drive/MyDrive/comparison_detector_models_v2')\n",
    "\n",
    "print(\"üìÅ Available models:\")\n",
    "print(\"-\" * 60)\n",
    "for f in model_dir.glob('*.h5'):\n",
    "    size_mb = f.stat().st_size / (1024*1024)\n",
    "    print(f\"  {f.name}: {size_mb:.1f} MB\")\n",
    "\n",
    "for f in model_dir.glob('*.keras'):\n",
    "    size_mb = f.stat().st_size / (1024*1024)\n",
    "    print(f\"  {f.name}: {size_mb:.1f} MB\")\n",
    "\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3dcf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model_path = '/content/drive/MyDrive/comparison_detector_models_v2/best_model.h5'\n",
    "# Ho·∫∑c d√πng final model:\n",
    "# model_path = '/content/drive/MyDrive/comparison_detector_models_v2/final_model.keras'\n",
    "\n",
    "print(f\"üì¶ Loading model: {model_path}\")\n",
    "model = tf.keras.models.load_model(model_path)\n",
    "print(\"‚úÖ Model loaded!\")\n",
    "print(f\"   Input shape: {model.input_shape}\")\n",
    "print(f\"   Output shape: {model.output_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b83f4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model summary (optional)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3362c08d",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Load Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a15ab87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.loader_tf2 import build_dataset\n",
    "from configs.config_v2 import IMAGE_SIZE, NUM_CLASSES, BATCH_SIZE\n",
    "\n",
    "# Load test dataset\n",
    "test_paths = ['/content/drive/MyDrive/content/data/tct/test.tfrecord']\n",
    "\n",
    "print(f\"üìä Loading test dataset from: {test_paths[0]}\")\n",
    "test_ds = build_dataset(\n",
    "    test_paths,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    is_training=False\n",
    ")\n",
    "\n",
    "# Extract labels for classification\n",
    "def extract_label(img, tgt):\n",
    "    return img, tgt['labels'][:, 0]\n",
    "\n",
    "test_ds_labeled = test_ds.map(extract_label)\n",
    "\n",
    "print(\"‚úÖ Test dataset loaded!\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Image size: {IMAGE_SIZE}\")\n",
    "print(f\"   Num classes: {NUM_CLASSES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d7da6e",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Quick Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251eab59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set (100 batches)\n",
    "print(\"‚öôÔ∏è  Evaluating model on test set...\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "results = model.evaluate(test_ds_labeled.take(100), verbose=1)\n",
    "\n",
    "test_loss = results[0]\n",
    "test_accuracy = results[1]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä TEST RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Test Loss:     {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56c27ce",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Detailed Analysis with Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6df6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Collect all predictions and ground truth\n",
    "print(\"üîÆ Collecting predictions...\")\n",
    "\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "all_images = []\n",
    "\n",
    "num_batches = 100  # Test on 100 batches\n",
    "\n",
    "for i, (images, labels) in enumerate(test_ds_labeled.take(num_batches)):\n",
    "    if i % 10 == 0:\n",
    "        print(f\"  Processing batch {i}/{num_batches}...\")\n",
    "    \n",
    "    preds = model.predict(images, verbose=0)\n",
    "    all_predictions.extend(np.argmax(preds, axis=1))\n",
    "    all_true_labels.extend(labels.numpy().astype(int))\n",
    "    all_images.extend(images.numpy())\n",
    "\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_true_labels = np.array(all_true_labels)\n",
    "all_images = np.array(all_images)\n",
    "\n",
    "print(f\"\\n‚úÖ Processed {len(all_predictions)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107b9fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = np.mean(all_predictions == all_true_labels)\n",
    "print(f\"\\nüìä Overall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"   Correct: {np.sum(all_predictions == all_true_labels)}\")\n",
    "print(f\"   Incorrect: {np.sum(all_predictions != all_true_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3795e226",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45329ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üìã CLASSIFICATION REPORT\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(\n",
    "    all_true_labels,\n",
    "    all_predictions,\n",
    "    target_names=[f\"Class {i}\" for i in range(NUM_CLASSES)],\n",
    "    zero_division=0\n",
    "))\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c27eb2",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e9db7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(all_true_labels, all_predictions)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=[f\"C{i}\" for i in range(NUM_CLASSES)],\n",
    "            yticklabels=[f\"C{i}\" for i in range(NUM_CLASSES)])\n",
    "plt.title('Confusion Matrix', fontsize=16)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save to Drive\n",
    "save_path = '/content/drive/MyDrive/comparison_detector_models_v2/confusion_matrix.png'\n",
    "plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"\\n‚úÖ Confusion matrix saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe9c2a7",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Visualize Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48457f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get correct and incorrect predictions\n",
    "correct_mask = all_predictions == all_true_labels\n",
    "incorrect_mask = ~correct_mask\n",
    "\n",
    "correct_indices = np.where(correct_mask)[0]\n",
    "incorrect_indices = np.where(incorrect_mask)[0]\n",
    "\n",
    "print(f\"‚úÖ Correct predictions: {len(correct_indices)}\")\n",
    "print(f\"‚ùå Incorrect predictions: {len(incorrect_indices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b47c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CORRECT predictions\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "fig.suptitle('‚úÖ CORRECT PREDICTIONS (Sample)', fontsize=16, color='green')\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < len(correct_indices):\n",
    "        idx = correct_indices[i]\n",
    "        ax.imshow(all_images[idx])\n",
    "        ax.set_title(f\"True: {all_true_labels[idx]}, Pred: {all_predictions[idx]}\",\n",
    "                    color='green', fontsize=10, fontweight='bold')\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save\n",
    "save_path = '/content/drive/MyDrive/comparison_detector_models_v2/correct_predictions.png'\n",
    "plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"‚úÖ Saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161c69e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize INCORRECT predictions\n",
    "if len(incorrect_indices) > 0:\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    fig.suptitle('‚ùå INCORRECT PREDICTIONS (Sample)', fontsize=16, color='red')\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < min(8, len(incorrect_indices)):\n",
    "            idx = incorrect_indices[i]\n",
    "            ax.imshow(all_images[idx])\n",
    "            ax.set_title(f\"True: {all_true_labels[idx]}, Pred: {all_predictions[idx]}\",\n",
    "                        color='red', fontsize=10, fontweight='bold')\n",
    "            ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Save\n",
    "    save_path = '/content/drive/MyDrive/comparison_detector_models_v2/incorrect_predictions.png'\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"‚úÖ Saved to: {save_path}\")\n",
    "else:\n",
    "    print(\"üéâ Perfect accuracy! No incorrect predictions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a184df8",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Class Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fd19f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot class distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# True labels distribution\n",
    "unique_true, counts_true = np.unique(all_true_labels, return_counts=True)\n",
    "ax1.bar(unique_true, counts_true, color='skyblue', edgecolor='black')\n",
    "ax1.set_title('True Label Distribution', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Class', fontsize=12)\n",
    "ax1.set_ylabel('Count', fontsize=12)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Predicted labels distribution\n",
    "unique_pred, counts_pred = np.unique(all_predictions, return_counts=True)\n",
    "ax2.bar(unique_pred, counts_pred, color='salmon', edgecolor='black')\n",
    "ax2.set_title('Predicted Label Distribution', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Class', fontsize=12)\n",
    "ax2.set_ylabel('Count', fontsize=12)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save\n",
    "save_path = '/content/drive/MyDrive/comparison_detector_models_v2/class_distribution.png'\n",
    "plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"‚úÖ Saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b277e6f",
   "metadata": {},
   "source": [
    "## üîü Per-Class Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b87969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate per-class accuracy\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä PER-CLASS ACCURACY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class_accuracies = []\n",
    "class_labels = []\n",
    "\n",
    "for i in range(NUM_CLASSES):\n",
    "    mask = all_true_labels == i\n",
    "    if np.sum(mask) > 0:\n",
    "        class_acc = np.mean(all_predictions[mask] == all_true_labels[mask])\n",
    "        total = np.sum(mask)\n",
    "        correct = np.sum(all_predictions[mask] == all_true_labels[mask])\n",
    "        \n",
    "        class_accuracies.append(class_acc)\n",
    "        class_labels.append(i)\n",
    "        \n",
    "        print(f\"Class {i:2d}: {class_acc:.4f} ({class_acc*100:5.2f}%) - {correct}/{total} correct\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4851246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot per-class accuracy\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(class_labels, class_accuracies, color='teal', edgecolor='black', alpha=0.7)\n",
    "\n",
    "# Color bars based on accuracy\n",
    "for i, (bar, acc) in enumerate(zip(bars, class_accuracies)):\n",
    "    if acc >= 0.9:\n",
    "        bar.set_color('green')\n",
    "    elif acc >= 0.7:\n",
    "        bar.set_color('orange')\n",
    "    else:\n",
    "        bar.set_color('red')\n",
    "\n",
    "plt.title('Per-Class Accuracy', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Class', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.ylim(0, 1.05)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.axhline(y=accuracy, color='black', linestyle='--', linewidth=2, label=f'Overall: {accuracy:.3f}')\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save\n",
    "save_path = '/content/drive/MyDrive/comparison_detector_models_v2/per_class_accuracy.png'\n",
    "plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"‚úÖ Saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ac0c91",
   "metadata": {},
   "source": [
    "## üíæ Save Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ca18d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Prepare results dictionary\n",
    "results_dict = {\n",
    "    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'model_path': model_path,\n",
    "    'total_images_tested': len(all_predictions),\n",
    "    'overall_accuracy': float(accuracy),\n",
    "    'test_loss': float(test_loss),\n",
    "    'num_correct': int(np.sum(all_predictions == all_true_labels)),\n",
    "    'num_incorrect': int(np.sum(all_predictions != all_true_labels)),\n",
    "    'per_class_accuracy': {},\n",
    "    'confusion_matrix': cm.tolist()\n",
    "}\n",
    "\n",
    "# Add per-class accuracy\n",
    "for i in range(NUM_CLASSES):\n",
    "    mask = all_true_labels == i\n",
    "    if np.sum(mask) > 0:\n",
    "        class_acc = np.mean(all_predictions[mask] == all_true_labels[mask])\n",
    "        results_dict['per_class_accuracy'][f'class_{i}'] = {\n",
    "            'accuracy': float(class_acc),\n",
    "            'total_samples': int(np.sum(mask)),\n",
    "            'correct_predictions': int(np.sum(all_predictions[mask] == all_true_labels[mask]))\n",
    "        }\n",
    "\n",
    "# Save to JSON\n",
    "save_path = '/content/drive/MyDrive/comparison_detector_models_v2/test_results.json'\n",
    "with open(save_path, 'w') as f:\n",
    "    json.dump(results_dict, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Test results saved to: {save_path}\")\n",
    "print(f\"\\nüìÑ Results preview:\")\n",
    "print(json.dumps(results_dict, indent=2)[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cc7a1b",
   "metadata": {},
   "source": [
    "## üìä Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbae7b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ TESTING COMPLETED!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"  Model: {Path(model_path).name}\")\n",
    "print(f\"  Images tested: {len(all_predictions)}\")\n",
    "print(f\"  Overall accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"  Test loss: {test_loss:.4f}\")\n",
    "print(f\"\\nüìÅ Files saved in Drive:\")\n",
    "print(f\"  - confusion_matrix.png\")\n",
    "print(f\"  - correct_predictions.png\")\n",
    "if len(incorrect_indices) > 0:\n",
    "    print(f\"  - incorrect_predictions.png\")\n",
    "print(f\"  - class_distribution.png\")\n",
    "print(f\"  - per_class_accuracy.png\")\n",
    "print(f\"  - test_results.json\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ All visualizations and results saved to Google Drive!\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
