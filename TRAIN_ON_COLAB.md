# üöÄ H∆∞·ªõng d·∫´n Training ComparisonDetector tr√™n Google Colab

## B∆∞·ªõc 1: M·ªü Google Colab

1. Truy c·∫≠p: https://colab.research.google.com/
2. T·∫°o notebook m·ªõi: File ‚Üí New notebook
3. Ch·ªçn GPU Runtime: Runtime ‚Üí Change runtime type ‚Üí GPU (T4 ho·∫∑c cao h∆°n)

---

## B∆∞·ªõc 2: Ki·ªÉm tra GPU

```python
# Cell 1: Ki·ªÉm tra GPU
!nvidia-smi

import tensorflow as tf
print(f"TensorFlow version: {tf.__version__}")
print(f"GPU devices: {tf.config.list_physical_devices('GPU')}")
print(f"Num GPUs Available: {len(tf.config.list_physical_devices('GPU'))}")
```

---

## B∆∞·ªõc 3: Mount Google Drive v√† Clone Repository

```python
# Cell 2: Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')
print("‚úì Google Drive mounted")
```

```python
# Cell 3: Clone repository
!git clone https://github.com/dungdinhhaha/AIDetect.git /content/ComparisonDetector
%cd /content/ComparisonDetector
!git pull
!ls -la
```

---

## B∆∞·ªõc 4: Chu·∫©n b·ªã d·ªØ li·ªáu

### Option A: Upload TFRecord t·ª´ Google Drive

```python
# Cell 4A: Copy TFRecord t·ª´ Drive
!mkdir -p /content/data/tct

# THAY ƒê·ªîI ƒë∆∞·ªùng d·∫´n n√†y theo v·ªã tr√≠ data c·ªßa b·∫°n tr√™n Drive
!cp /content/drive/MyDrive/TCT_Data/*.tfrecord /content/data/tct/ 2>/dev/null || echo "No TFRecords found"

# Ki·ªÉm tra
!ls -lh /content/data/tct/
```

### Option B: T·∫°o Dummy Data ƒë·ªÉ Test

```python
# Cell 4B: T·∫°o dummy TFRecord
import tensorflow as tf
import numpy as np

def create_dummy_tfrecord(output_path, num_samples=100):
    """T·∫°o TFRecord gi·∫£ v·ªõi image v√† bounding boxes"""
    with tf.io.TFRecordWriter(output_path) as writer:
        for i in range(num_samples):
            # Dummy image (640x640x3)
            img = np.random.randint(0, 255, (640, 640, 3), dtype=np.uint8)
            img_bytes = tf.io.encode_jpeg(img).numpy()
            
            # Dummy boxes v√† labels
            num_boxes = np.random.randint(1, 5)
            boxes = np.random.rand(num_boxes, 4).astype(np.float32)
            labels = np.random.randint(1, 12, num_boxes, dtype=np.int64)
            
            feature = {
                'image': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_bytes])),
                'boxes': tf.train.Feature(float_list=tf.train.FloatList(value=boxes.flatten())),
                'labels': tf.train.Feature(int64_list=tf.train.Int64List(value=labels)),
                'num_boxes': tf.train.Feature(int64_list=tf.train.Int64List(value=[num_boxes])),
            }
            example = tf.train.Example(features=tf.train.Features(feature=feature))
            writer.write(example.SerializeToString())
    print(f"‚úì Created {output_path} with {num_samples} samples")

# T·∫°o train v√† test TFRecord
!mkdir -p /content/data/tct
create_dummy_tfrecord('/content/data/tct/train.tfrecord', num_samples=500)
create_dummy_tfrecord('/content/data/tct/test.tfrecord', num_samples=100)

!ls -lh /content/data/tct/
```

---

## B∆∞·ªõc 5: Xem v√† T√πy ch·ªânh Config

```python
# Cell 5: Xem config hi·ªán t·∫°i
from configs.config_v2 import ConfigV2

cfg = ConfigV2()
print("üìã Current Configuration:")
print(f"  Data Dir: {cfg.DATA_DIR}")
print(f"  Model Dir: {cfg.MODEL_DIR}")
print(f"  Batch Size: {cfg.BATCH_SIZE}")
print(f"  Epochs: {cfg.EPOCHS}")
print(f"  Learning Rate: {cfg.LEARNING_RATE}")
print(f"  Backbone: {cfg.BACKBONE}")
print(f"  Image Size: {cfg.IMAGE_SIZE}")
print(f"  Num Classes: {cfg.NUM_CLASSES}")
```

```python
# Cell 6: Override config (optional)
cfg.BATCH_SIZE = 1  # Gi·∫£m batch size n·∫øu GPU nh·ªè
cfg.EPOCHS = 10     # S·ªë epochs mu·ªën train
cfg.LEARNING_RATE = 5e-4  # Fine-tune learning rate

print("‚úì Config updated for Colab")
```

---

## B∆∞·ªõc 6: Test Data Pipeline v√† Model

```python
# Cell 7: Test data loader
import os
from data.loader_tf2 import build_dataset

tfrecord_paths = tf.io.gfile.glob(os.path.join(cfg.DATA_DIR, '*.tfrecord'))
print(f"Found {len(tfrecord_paths)} TFRecord files")

if tfrecord_paths:
    ds = build_dataset(tfrecord_paths, image_size=cfg.IMAGE_SIZE, batch_size=cfg.BATCH_SIZE)
    
    for images, targets in ds.take(1):
        print(f"‚úì Data pipeline working")
        print(f"  Images shape: {images.shape}")
        print(f"  Targets keys: {targets.keys()}")
else:
    print("‚ö† No TFRecords found - will use dummy dataset")
```

```python
# Cell 8: Test model architecture
from models.detector import ComparisonDetector

detector = ComparisonDetector(
    num_classes=cfg.NUM_CLASSES,
    backbone_name=cfg.BACKBONE,
    backbone_weights=cfg.BACKBONE_WEIGHTS
)

dummy_input = tf.random.uniform((1, 640, 640, 3))
boxes, scores = detector(dummy_input, training=False)

print(f"‚úì Model test passed")
print(f"  Output boxes: {boxes.shape}")
print(f"  Output scores: {scores.shape}")
```

---

## B∆∞·ªõc 7: B·∫Øt ƒë·∫ßu Training üöÄ

### Option 1: Ch·∫°y script training

```python
# Cell 9A: Ch·∫°y train_keras.py
!python train_keras.py
```

### Option 2: Train tr·ª±c ti·∫øp trong notebook

```python
# Cell 9B: Train trong notebook
import os
import tensorflow as tf
from tensorflow.keras import optimizers
from configs.config_v2 import ConfigV2
from data.loader_tf2 import build_dataset
from models.backbone_keras import build_backbone

# Config
cfg = ConfigV2()
os.makedirs(cfg.MODEL_DIR, exist_ok=True)
os.makedirs(cfg.LOG_DIR, exist_ok=True)
os.makedirs(cfg.CHECKPOINT_DIR, exist_ok=True)

# Distribution strategy
strategy = tf.distribute.MirroredStrategy() if cfg.USE_DISTRIBUTE else tf.distribute.get_strategy()

with strategy.scope():
    # Build model
    backbone = build_backbone(cfg.BACKBONE, cfg.BACKBONE_WEIGHTS)
    inputs = backbone.input
    features = backbone(inputs)[-1]
    x = tf.keras.layers.GlobalAveragePooling2D()(features)
    outputs = tf.keras.layers.Dense(cfg.NUM_CLASSES, activation='softmax')(x)
    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='comparison_detector_v2')
    
    # Compile
    opt = optimizers.SGD(learning_rate=cfg.LEARNING_RATE, momentum=cfg.MOMENTUM)
    model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Dataset
tfrecord_paths = tf.io.gfile.glob(os.path.join(cfg.DATA_DIR, '*.tfrecord'))
if not tfrecord_paths:
    print('‚ö† No TFRecords found, using dummy dataset...')
    dummy_images = tf.random.uniform((cfg.BATCH_SIZE, cfg.IMAGE_SIZE[0], cfg.IMAGE_SIZE[1], 3))
    dummy_labels = tf.random.uniform((cfg.BATCH_SIZE,), minval=0, maxval=cfg.NUM_CLASSES, dtype=tf.int32)
    ds = tf.data.Dataset.from_tensor_slices((dummy_images, dummy_labels)).repeat().batch(cfg.BATCH_SIZE)
else:
    ds = build_dataset(tfrecord_paths, image_size=cfg.IMAGE_SIZE, batch_size=cfg.BATCH_SIZE)
    ds = ds.map(lambda img, tgt: (img, tf.zeros((), dtype=tf.int32))).repeat()

# Callbacks
ckpt_cb = tf.keras.callbacks.ModelCheckpoint(
    filepath=os.path.join(cfg.CHECKPOINT_DIR, 'ckpt_{epoch:02d}.weights.h5'),
    save_weights_only=True,
    save_freq='epoch'
)
tb_cb = tf.keras.callbacks.TensorBoard(log_dir=cfg.LOG_DIR)

# Train
print("\nüöÄ Starting training...\n")
history = model.fit(
    ds, 
    epochs=cfg.EPOCHS, 
    steps_per_epoch=100,  # ƒêi·ªÅu ch·ªânh theo dataset size
    callbacks=[ckpt_cb, tb_cb]
)

# Save
model.save(os.path.join(cfg.MODEL_DIR, 'model.keras'))
print('\n‚úÖ Training completed!')
print(f'Model saved to: {cfg.MODEL_DIR}/model.keras')
```

---

## B∆∞·ªõc 8: TensorBoard Monitoring (Optional)

```python
# Cell 10: Load TensorBoard
%load_ext tensorboard
%tensorboard --logdir /content/drive/MyDrive/comparison_detector_models_v2/logs
```

---

## B∆∞·ªõc 9: ƒê√°nh gi√° Model

```python
# Cell 11: Load v√† test model
model_path = os.path.join(cfg.MODEL_DIR, 'model.keras')
if os.path.exists(model_path):
    loaded_model = tf.keras.models.load_model(model_path)
    print(f"‚úì Model loaded from {model_path}")
    
    # Test inference
    test_img = tf.random.uniform((1, 640, 640, 3))
    pred = loaded_model(test_img, training=False)
    print(f"  Prediction shape: {pred.shape}")
    print(f"  Predicted class: {tf.argmax(pred[0]).numpy()}")
else:
    print(f"‚ö† Model not found at {model_path}")
```

---

## B∆∞·ªõc 10: Download Model v·ªÅ Local

```python
# Cell 12: Zip v√† download
!cd /content/drive/MyDrive && zip -r comparison_detector_models_v2.zip comparison_detector_models_v2/

from google.colab import files
files.download('/content/drive/MyDrive/comparison_detector_models_v2.zip')
```

---

## üìù Troubleshooting

### ‚ùå Out of Memory
```python
# Gi·∫£m batch size v√† image size
cfg.BATCH_SIZE = 1
cfg.IMAGE_SIZE = (512, 512)
cfg.BACKBONE = 'resnet50'  # Thay v√¨ resnet101
```

### ‚ùå No TFRecords Found
- Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n trong Cell 4A
- Ho·∫∑c d√πng Option B ƒë·ªÉ t·∫°o dummy data

### ‚ùå Model Not Converging
```python
# Gi·∫£m learning rate
cfg.LEARNING_RATE = 1e-4

# Xem TensorBoard ƒë·ªÉ debug
%tensorboard --logdir /content/drive/MyDrive/comparison_detector_models_v2/logs
```

### ‚ùå GPU Disconnected
- Colab free c√≥ gi·ªõi h·∫°n th·ªùi gian
- L∆∞u checkpoint th∆∞·ªùng xuy√™n
- N√¢ng c·∫•p Colab Pro n·∫øu c·∫ßn train l√¢u

---

## üíæ Model ƒë∆∞·ª£c l∆∞u t·∫°i:

- **Google Drive**: `/content/drive/MyDrive/comparison_detector_models_v2/`
- **Checkpoints**: M·ªói epoch t·∫°i `/checkpoints/`
- **TensorBoard logs**: `/logs/`
- **Final model**: `model.keras`

---

## üìä Theo d√µi Training:

1. **Terminal output**: Loss v√† accuracy m·ªói epoch
2. **TensorBoard**: Graphs chi ti·∫øt
3. **Google Drive**: Auto-save checkpoints

---

**Repository**: https://github.com/dungdinhhaha/AIDetect  
**Author**: dungdinhhaha | dungdinh542004@gmail.com  
**Email**: dungdinh542004@gmail.com

---

## üéØ Quick Start (Copy-Paste t·∫•t c·∫£):

```python
# Setup nhanh - paste t·∫•t c·∫£ v√†o 1 cell
!git clone https://github.com/dungdinhhaha/AIDetect.git /content/ComparisonDetector
%cd /content/ComparisonDetector

from google.colab import drive
drive.mount('/content/drive')

!mkdir -p /content/data/tct
# Uncomment d√≤ng d∆∞·ªõi n·∫øu c√≥ data th·∫≠t
# !cp /content/drive/MyDrive/TCT_Data/*.tfrecord /content/data/tct/

# Train
!python train_keras.py
```

Copy code tr√™n v√†o Google Colab v√† ch·∫°y ƒë·ªÉ b·∫Øt ƒë·∫ßu training ngay! üöÄ
