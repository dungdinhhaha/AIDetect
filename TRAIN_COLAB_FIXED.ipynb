{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "143036ad",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Setup & Mount Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e900c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/dungdinhhaha/AIDetect.git /content/ComparisonDetector 2>/dev/null || echo 'Repo already cloned'\n",
    "%cd /content/ComparisonDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95216cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q tensorflow==2.19.0 matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788ad87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"âœ… Google Drive mounted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e7c677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import tensorflow as tf\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f\"GPU available: {len(gpus) > 0}\")\n",
    "if gpus:\n",
    "    print(f\"GPU devices: {[gpu.name for gpu in gpus]}\")\n",
    "else:\n",
    "    print(\"âš ï¸  No GPU detected! Go to Runtime > Change runtime type > Select GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1692662",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Verify Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d2c479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if data exists\n",
    "data_dir = Path('/content/drive/MyDrive/content/data/tct')\n",
    "if data_dir.exists():\n",
    "    files = list(data_dir.glob('*.tfrecord'))\n",
    "    print(f\"âœ… Found {len(files)} TFRecord files:\")\n",
    "    for f in files:\n",
    "        size_gb = f.stat().st_size / (1024**3)\n",
    "        print(f\"   - {f.name}: {size_gb:.2f} GB\")\n",
    "else:\n",
    "    print(f\"âŒ Data directory not found: {data_dir}\")\n",
    "    print(\"Make sure to upload data to Google Drive first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e141b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data loading\n",
    "from configs.config_v2 import IMAGE_SIZE, NUM_CLASSES, BATCH_SIZE\n",
    "from data.loader_tf2 import build_dataset\n",
    "\n",
    "print(\"Testing data pipeline...\")\n",
    "print(f\"Image size: {IMAGE_SIZE}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Num classes: {NUM_CLASSES}\")\n",
    "\n",
    "test_paths = ['/content/drive/MyDrive/content/data/tct/train.tfrecord']\n",
    "ds = build_dataset(test_paths, image_size=IMAGE_SIZE, batch_size=BATCH_SIZE, is_training=False)\n",
    "\n",
    "for images, targets in ds.take(1):\n",
    "    print(f\"\\nâœ… Data pipeline working!\")\n",
    "    print(f\"   Images shape: {images.shape}\")\n",
    "    print(f\"   Labels shape: {targets['labels'].shape}\")\n",
    "    print(f\"   Boxes shape: {targets['boxes'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352cd0c8",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ ğŸš€ Start Training (Fixed Version)\n",
    "\n",
    "**Key improvements:**\n",
    "- `.repeat()` added - dataset won't run out of data\n",
    "- Dynamic `steps_per_epoch` - calculated from actual data size\n",
    "- Better callbacks - saves best model, reduces LR on plateau\n",
    "- Informative logging - shows training progress clearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c338899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "print(\"ğŸš€ Starting training...\")\n",
    "print(\"Expected duration: ~4-6 hours for 20 epochs on Tesla T4\\n\")\n",
    "\n",
    "!python train_keras.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7e9d91",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Monitor Training (TensorBoard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ab6388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TensorBoard extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Display TensorBoard\n",
    "log_dir = '/content/drive/MyDrive/comparison_detector_models_v2/logs'\n",
    "%tensorboard --logdir {log_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d36095",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ Check Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897dc2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "ckpt_dir = Path('/content/drive/MyDrive/comparison_detector_models_v2/checkpoints')\n",
    "\n",
    "if ckpt_dir.exists():\n",
    "    ckpts = sorted(list(ckpt_dir.glob('ckpt_*.weights.h5')), key=lambda x: int(x.stem.split('_')[1]))\n",
    "    print(f\"âœ… Found {len(ckpts)} checkpoints:\")\n",
    "    for ckpt in ckpts[-5:]:  # Show last 5\n",
    "        size_mb = ckpt.stat().st_size / (1024*1024)\n",
    "        print(f\"   - {ckpt.name}: {size_mb:.1f} MB\")\n",
    "else:\n",
    "    print(\"No checkpoints found yet. Training may still be in progress.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c4980a",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ Resume Training (if interrupted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c643cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If training was interrupted, run this to resume\n",
    "print(\"Resuming training from checkpoint...\")\n",
    "!python resume_training.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0995c939",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ Post-Training: Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af254f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "model_dir = Path('/content/drive/MyDrive/comparison_detector_models_v2')\n",
    "\n",
    "print(\"ğŸ“¦ Post-Training: Saving Models & Archives\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create archive folder\n",
    "timestamp = datetime.now().strftime('%Y_%m_%d_%H%M%S')\n",
    "archive_dir = model_dir / f'archive_{timestamp}'\n",
    "archive_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Copy best model\n",
    "best_model_src = model_dir / 'best_model.h5'\n",
    "if best_model_src.exists():\n",
    "    shutil.copy(best_model_src, archive_dir / 'best_model.h5')\n",
    "    size_mb = best_model_src.stat().st_size / (1024*1024)\n",
    "    print(f\"âœ… Best model archived: {size_mb:.1f} MB\")\n",
    "\n",
    "# Copy final model\n",
    "final_model_src = model_dir / 'final_model.keras'\n",
    "if final_model_src.exists():\n",
    "    shutil.copy(final_model_src, archive_dir / 'final_model.keras')\n",
    "    size_mb = final_model_src.stat().st_size / (1024*1024)\n",
    "    print(f\"âœ… Final model archived: {size_mb:.1f} MB\")\n",
    "\n",
    "# Create metadata\n",
    "metadata = {\n",
    "    'timestamp': timestamp,\n",
    "    'date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'framework': 'TensorFlow',\n",
    "    'tf_version': '2.19.0',\n",
    "    'epochs': 20,\n",
    "    'batch_size': 2,\n",
    "    'image_size': [640, 640],\n",
    "    'num_classes': 12,\n",
    "    'backbone': 'resnet50'\n",
    "}\n",
    "\n",
    "with open(archive_dir / 'metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ… Archive saved to: {archive_dir}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d150850",
   "metadata": {},
   "source": [
    "## 8ï¸âƒ£ Export Models for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc468b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "\n",
    "model_dir = Path('/content/drive/MyDrive/comparison_detector_models_v2')\n",
    "best_model_path = model_dir / 'best_model.h5'\n",
    "\n",
    "if best_model_path.exists():\n",
    "    print(\"ğŸ“¤ Exporting models...\\n\")\n",
    "    \n",
    "    # Load model\n",
    "    model = tf.keras.models.load_model(str(best_model_path))\n",
    "    \n",
    "    # 1. TensorFlow SavedModel (for FastAPI/TF Serving)\n",
    "    saved_model_dir = model_dir / 'saved_model'\n",
    "    model.save(str(saved_model_dir), save_format='tf')\n",
    "    print(f\"âœ… SavedModel exported to: saved_model/\")\n",
    "    \n",
    "    # 2. TF Lite (for mobile/edge devices - optional)\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    tflite_model = converter.convert()\n",
    "    \n",
    "    tflite_path = model_dir / 'model_quantized.tflite'\n",
    "    with open(tflite_path, 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "    print(f\"âœ… TFLite model exported: model_quantized.tflite\")\n",
    "    \n",
    "    # Show file sizes\n",
    "    print(\"\\nğŸ“Š Model sizes:\")\n",
    "    for name, path in [\n",
    "        ('Best Model (H5)', model_dir / 'best_model.h5'),\n",
    "        ('Final Model (Keras)', model_dir / 'final_model.keras'),\n",
    "        ('TFLite Quantized', tflite_path)\n",
    "    ]:\n",
    "        if path.exists():\n",
    "            size_mb = path.stat().st_size / (1024*1024)\n",
    "            print(f\"  {name}: {size_mb:.1f} MB\")\n",
    "else:\n",
    "    print(\"âŒ Best model not found. Training may not be complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92665cde",
   "metadata": {},
   "source": [
    "## 9ï¸âƒ£ Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3665a22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘          âœ… TRAINING & EXPORT COMPLETED!                    â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "ğŸ“‹ What to do next:\n",
    "\n",
    "1ï¸âƒ£  EVALUATE MODEL\n",
    "   â†’ Open TEST_MODEL_COLAB.ipynb\n",
    "   â†’ Get accuracy, precision, recall, F1 scores\n",
    "   â†’ Visualize confusion matrix & predictions\n",
    "\n",
    "2ï¸âƒ£  DOWNLOAD MODELS\n",
    "   â†’ Google Drive: MyDrive/comparison_detector_models_v2/\n",
    "   â†’ Download best_model.h5 or final_model.keras\n",
    "   â†’ Download saved_model/ folder for FastAPI\n",
    "\n",
    "3ï¸âƒ£  DEPLOY LOCALLY\n",
    "   â†’ Copy models to d:\\\\ComparisonDetector\\\\trained_models\\\\\n",
    "   â†’ Run: python test_model_simple.py\n",
    "   â†’ Or: python test_model_advanced.py\n",
    "\n",
    "4ï¸âƒ£  IMPROVE MODEL\n",
    "   â†’ Check ROADMAP_Cáº¢I_THIá»†N_Äáº T_Má»¨C_LÃ‚M_SÃ€NG.md\n",
    "   â†’ Data augmentation\n",
    "   â†’ Fine-tune hyperparameters\n",
    "   â†’ Add regularization\n",
    "\n",
    "ğŸ“š AVAILABLE GUIDES:\n",
    "   - HÆ¯á»šNG_DáºªN_SAU_KHI_TRAIN_COLAB.md (Post-training)\n",
    "   - Cáº¢I_THIá»†N_Äá»˜_CHÃNH_XÃC.md (Accuracy improvements)\n",
    "   - ROADMAP_Cáº¢I_THIá»†N_Äáº T_Má»¨C_LÃ‚M_SÃ€NG.md (Clinical-grade guide)\n",
    "\n",
    "ğŸ‰ Good luck with your model!\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
